\documentclass[a4paper, 12pt]{article}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{mathtools}
\usepackage{hyperref}
\usepackage{caption}
\usepackage{subcaption}
\def\code#1{\texttt{#1}}
\def\f#1{Figure \ref{fig:#1}}
\begin{document}

\title{\vspace{4.0cm}Applied GPU Programming - Assignment IV\\
\large DD2360 HT20}
\author{Pontus Asp}
\date{\today}
\maketitle
\thispagestyle{empty}
\pagenumbering{roman}
\newpage

\clearpage
\pagenumbering{arabic}

% Write here ->
\section{Git repository}
I uploaded my git repository to GitHub. I use the same git repository for the entire course but the folder structure requested is still followed under the root folder. I also have 2 extra directories, one for this report and one where I have code from lectures. I also was not sure if Exercise 4 should have been separate or included in this report so I did both, the standalone Exercise\_4.pdf can be found under the ex\_4 folder.
\\\\
Here is the link to my git repository:\\
\url{https://github.com/pontusasp/kth-dd2360/tree/master/Assignment_4}

% List and explain your extensions to the basic template provided by the course.
\section{Exercise 1}
The first thing I did when extending the template was writing a \code{helloworld} kernel in the char array \code{mykernel}. The reason for why I wrote the kernel is probably pretty self explanatory, but why it is in a char array might be more confusing. The kernel is written in a string since it will (can) be compiled at runtime by OpenCL, so that the kernel is compiled to be able to be used on the devices on the host system. Therefore it is in a char array so that the source code can be passed to the OpenCL API to be compiled and set up.
After writing the kernel I started writing code in the \code{main} function between the comments specifying where to insert my code. The first thing I added here was a call to the \code{clCreateProgramWithSource} which I passed some arguments to, and the two most significant ones where my context and a pointer to my source code. What happens next is that \code{clCreateProgramWithSource} loads the program source code and stores it in a program object, which also gets associated with the OpenCL context I gave it, and then returns it.
However the program is not ready to be used yet, so what I did next was calling \code{clBuildProgram} which I passed the program object to. This function will then compile (and link) my program and update my program object.
After this point you might think that your program is ready to be executed, but one more API call is needed. To be honest, I could not find information of why this step is needed but I am assuming that \code{clBuildProgram} simply compiles my executable program and the next step I did, \code{clCreateKernel} will take this program and prepare it to be launched on my OpenCL device, and also tell OpenCL the starting point of my program. Now when my program, or should I say kernel, is compiled and loaded we need to launch it on the device. When launching the kernel we need to specify the size of our working groups and the number of work items we want to use (on a GPU a work item is a thread). We do all this with \code{clEnqueueNDRangeKernel} which we also will give our command queue and kernel to, along with the number of work groups and items.
When running a kernel on OpenCL it is running asynchronously from our code so the next thing I did was call \code{clFinish} and give it our command queue. This function simply waits for OpenCL to finish all the commands that we have queued and then returns control to our code. We do this so that we will not exit our program before OpenCL is finished.

% Explain how you solved the issue when the ARRAY_SIZE is not a multiple of the block size. If you implemented timing in the code, vary ARRAY_SIZE from small to large, and explain how the execution time changes between GPU and CPU.
\section{Exercise 2}

% 1. Measure the execution time of the CPU version, varying the number of particles.
%
% 2. Measure the execution time of the GPU version, varying the number of particles, like in 1).
%   2.1. Include data copying time to and from in the measurement.
%   2.2. For each GPU particle configuration, vary the block size in the GPU version from 16, 32, â€¦, up to 256 threads per block.
%
% 3. Generate one or more performance figures based on your measurements in 1 and 2. Include it in the report with a description of the experimental setup (e.g., GPU used) and the observations obtained from the results. Explain how the execution time of the two version changes when the number of particles increases. Which block size configuration is optimal?
%
% 4. Currently, the particle mover is completely offloaded to the GPU, only with data transfer at the beginning and end of the simulation. If the simulation involves CPU dependent functions (i.e. particles need to be copied back and forth every time step), would your observation in 3) still holds? How would you expect the performance will change in terms of GPU execution? Make an educated guess by timing your kernels.
%
% Important note: If the execution times between the CPU and the GPU are very similar, try to increase the number of particles and the number of iterations per simulation even further.
%
\section{Bonus Exercise}


% WRITE STANDALONE FILE FIRST
\section{Exercise 4}
% WRITE STANDALONE FILE FIRST

% Write here <--

\end{document}



%\begin{figure}
%  \centering
%  \begin{subfigure}{.5\textwidth}
%    \centering
%    \includegraphics[width=1\linewidth]{graphs/ex_bonus_double_error.png}
%    \caption{Double Precision}
%    \label{fig:ex-single-double-error}
%  \end{subfigure}%
%  \begin{subfigure}{.5\textwidth}
%    \centering
%    \includegraphics[width=1\linewidth]{graphs/ex_bonus_single_error.png}
%    \caption{Single Precision}
%    \label{fig:ex-bonus-single-error}
%  \end{subfigure}
%  \caption{Graphs of error using double and single precision with different amounts of iterations and block sizes.}
%  \label{fig:fig:ex-bonus-error}
%\end{figure}